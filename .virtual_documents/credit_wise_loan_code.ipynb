


# Libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split


# Loading Data

df = pd.read_csv("loan_approval_data.csv")

# print(df.head(10))
print(df.info())
print(df.describe())





# separating columns of different datatypes values
categorical_cols = df.select_dtypes(include=["object"]).columns
numerical_cols = df.select_dtypes(include=["number"]).columns
#print(f"Categorical Columns: \n\n{categorical_cols}\n\n\nNumerical Columns: \n\n{numerical_cols}")

# importing SimpleImputer 
from sklearn.impute import SimpleImputer

# Defining Stratgy to numeric columns
num_imp = SimpleImputer(strategy="mean")
# Filling Missing Values of Numeric columns using num_imp Strategy
df[numerical_cols] = num_imp.fit_transform(df[numerical_cols])



# Defining Stratgy to categorical columns
cat_imp = SimpleImputer(strategy="most_frequent")
# Filling Missing Values of Numeric columns using num_imp Strategy
df[categorical_cols] = cat_imp.fit_transform(df[categorical_cols])






# Checking How Balanced Our Classes Are?

classes_count = df["Loan_Approved"].value_counts()

plt.pie(classes_count, labels=["No", "Yes"], autopct="%1.1f%%")
plt.title("Is Loan approved or not?")


# Analysing Catgories:

gender_count = df["Gender"].value_counts()

ax = sns.barplot(gender_count)
ax.bar_label(ax.containers[0])



edu_count = df["Education_Level"].value_counts()

ax = sns.barplot(edu_count)
ax.bar_label(ax.containers[1])



emp_count = df["Employment_Status"].value_counts()

ax = sns.barplot(emp_count)
ax.bar_label(ax.containers[2])
plt.xticks(rotation=30)


# Analyzing Income

fig, axes = plt.subplots(5, 2, figsize=(12, 20))

sns.histplot(
    data=df,
    x="Applicant_Income",
    bins=20,
    ax=axes[0, 0]
)
axes[0, 0].set_title("Applicants Income Ranges")

sns.histplot(
    data=df,
    x="Coapplicant_Income",
    bins=20,
    ax=axes[0, 1]
)
axes[0, 1].set_title("Coapplicants Income Ranges")

sns.histplot(
    data=df,
    x="Applicant_Income",
    hue="Loan_Approved",
    bins = 20,
    multiple="dodge",
    ax=axes[4, 0]
)
axes[4, 0].set_title("Loan Approval vs Applicant Income")


sns.histplot(
    data=df,
    x="Credit_Score",
    hue="Loan_Approved",
    bins = 20,
    multiple="dodge",
    ax=axes[4, 1]
)
axes[4, 1].set_title("Loan Approval vs Credit Score")


# Outliers Detection : Box Plot
sns.boxplot(
    data=df,
    x="Loan_Approved",
    y="Applicant_Income",
    ax=axes[1, 0]
)
axes[1, 0].set_title("Loan Approval vs Applicant Income")


sns.boxplot(
    data=df,
    x="Loan_Approved",
    y="Credit_Score",
    ax=axes[1, 1]
)
axes[1, 1].set_title("Loan Approval vs Credit Score")


sns.boxplot(
    data=df,
    x="Loan_Approved",
    y="DTI_Ratio",
    ax=axes[2, 0]
)
axes[2, 0].set_title("Loan Approval vs DTI Ratio")


sns.boxplot(
    data=df,
    x="Loan_Approved",
    y="Savings",
    ax=axes[2, 1]
)
axes[2, 1].set_title("Loan Approval vs Savings")


sns.boxplot(
    data=df,
    x="Loan_Approved",
    y="Age",
    ax=axes[3, 0]
)
axes[3, 0].set_title("Loan Approval vs Age")


sns.boxplot(
    data=df,
    x="Loan_Approved",
    y="Loan_Amount",
    ax=axes[3, 1]
)
axes[3, 1].set_title("Loan Approval vs Loan Amount")




plt.tight_layout()
plt.show()


# Removing Applicant_Id Column: Cause it is given by bank for
# unique identification of users 
# It is not relevent for model training

# df = df.drop(columns=["Applicant_ID"])
df.head()
df.info()





from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# LabelEncoder: For Ordinal data (Categories in columns have their label/Order)
le = LabelEncoder()
df["Education_Level"] = le.fit_transform(df["Education_Level"])
df["Loan_Approved"] = le.fit_transform(df["Loan_Approved"])


# OneHotEncoder: For Nominal data (Not any label all categories of columns are equal)
cols = ["Employment_Status", "Marital_Status", "Loan_Purpose", "Property_Area", "Gender", "Employer_Category"]

ohe = OneHotEncoder(drop="first", sparse_output=False, handle_unknown="ignore")

encoded = ohe.fit_transform(df[cols])

encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(cols), index=df.index)

df = pd.concat([df.drop(columns=cols), encoded_df], axis=1)





# Selecting all numeric columns
num_cols = df.select_dtypes(include="number")

# matrix of relation between all numeric columns
corr_matrix = num_cols.corr()

# Correlation Heatmap
plt.figure(figsize=(18, 10))
sns.heatmap(
    corr_matrix,
    annot=True,
    fmt=".2f",
    cmap="coolwarm"
)





X = df.drop(columns=["Loan_Approved"])
y = df["Loan_Approved"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Identify column types
num_cols = X_train.select_dtypes(include=["int64", "float64"]).columns
cat_cols = X_train.select_dtypes(include=["object", "category"]).columns

#
X_train_encoded = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)
X_test_encoded  = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)






# Scaling Data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)



df.info()








from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

log_model = LogisticRegression(max_iter=10000)
log_model.fit(X_train_scaled, y_train)

y_pred = log_model.predict(X_test_scaled)

# Evaluation
print("Logistic Regression Model")
print("Precision: ", precision_score(y_test, y_pred))
print("Recall: ", recall_score(y_test, y_pred))
print("F1 score: ", f1_score(y_test, y_pred))
print("Accuracy: ", accuracy_score(y_test, y_pred))
print("CM: ", confusion_matrix(y_test, y_pred))





from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=7)

knn_model.fit(X_train_scaled, y_train)

y_pred = knn_model.predict(X_test_scaled)

# Evaluation
print("KNN Model")
print("Precision: ", precision_score(y_test, y_pred))
print("Recall: ", recall_score(y_test, y_pred))
print("F1 score: ", f1_score(y_test, y_pred))
print("Accuracy: ", accuracy_score(y_test, y_pred))
print("CM: ", confusion_matrix(y_test, y_pred))





from sklearn.naive_bayes import GaussianNB

nb_model = KNeighborsClassifier()

nb_model.fit(X_train_scaled, y_train)

y_pred = nb_model.predict(X_test_scaled)

# Evaluation
print("Naive Bayes Model")
print("Precision: ", precision_score(y_test, y_pred))
print("Recall: ", recall_score(y_test, y_pred))
print("F1 score: ", f1_score(y_test, y_pred))
print("Accuracy: ", accuracy_score(y_test, y_pred))
print("CM: ", confusion_matrix(y_test, y_pred))





df["DTI_Ratio_sq"] = df["DTI_Ratio"] ** 2
df["Credit_Score_sq"] = df["Credit_Score"] ** 2

# Compressing Extreme(Skewed) Data to make them non-skewed
df["Applicant_Income_log"] = np.log1p(df["Applicant_Income"])

X = df.drop(columns=["Loan_Approved", "Credit_Score", "DTI_Ratio"])
y = df["Loan_Approved"]

# train-test-split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

#Feature Scaling
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)





log_model.fit(X_train_scaled, y_train)

y_pred = log_model.predict(X_test_scaled)

# Evaluation
print("Logistic Regression Model")
print("Precision: ", precision_score(y_test, y_pred))
print("Recall: ", recall_score(y_test, y_pred))
print("F1 score: ", f1_score(y_test, y_pred))
print("Accuracy: ", accuracy_score(y_test, y_pred))
print("CM: ", confusion_matrix(y_test, y_pred))





knn_model = KNeighborsClassifier(n_neighbors=7)

knn_model.fit(X_train_scaled, y_train)

y_pred = knn_model.predict(X_test_scaled)

# Evaluation
print("KNN Model")
print("Precision: ", precision_score(y_test, y_pred))
print("Recall: ", recall_score(y_test, y_pred))
print("F1 score: ", f1_score(y_test, y_pred))
print("Accuracy: ", accuracy_score(y_test, y_pred))
print("CM: ", confusion_matrix(y_test, y_pred))





nb_model.fit(X_train_scaled, y_train)

y_pred = nb_model.predict(X_test_scaled)

# Evaluation
print("Naive Bayes Model")
print("Precision: ", precision_score(y_test, y_pred))
print("Recall: ", recall_score(y_test, y_pred))
print("F1 score: ", f1_score(y_test, y_pred))
print("Accuracy: ", accuracy_score(y_test, y_pred))
print("CM: ", confusion_matrix(y_test, y_pred))


print("LOGISTIC REGRESSION is working well.")



